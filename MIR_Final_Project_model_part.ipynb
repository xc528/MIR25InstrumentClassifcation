{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In general, this notebook is designed to specifically work with how other teammates present their data. Do not attempt to use this notebook without previously prepared datasets either local or on the cloud, otherwise heavy modification will be needed to integrate previous data processing pipelines."
      ],
      "metadata": {
        "id": "hT_eItdZF26W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HYPERPARAMS"
      ],
      "metadata": {
        "id": "Qm-sltZLILKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nothing needs to be changed here, since we are not training, but we do specify we use cuda for workload,"
      ],
      "metadata": {
        "id": "8ykZePoAAQ_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "hyper = {\n",
        "    # ---- General ----\n",
        "    \"seed\": 1337,\n",
        "    \"device\": \"cuda\",             # used for PyTorch (heads & AudioMAE)\n",
        "    \"runtime_root\": \"/content\",\n",
        "    \"dataset_root\": \"medley_work\",\n",
        "    \"target_sr\": 16000,\n",
        "    \"use_amp\": True,\n",
        "    \"num_workers\": 4,\n",
        "\n",
        "    # ---- Splits (track-level stratified) ----\n",
        "    \"split_val_ratio\": 0.15,\n",
        "    \"split_test_ratio\": 0.15,\n",
        "\n",
        "    # ---- Augmentation ----\n",
        "    \"aug_reverb_prob\": 0.7,\n",
        "    \"aug_reverb_wet\": (0.1, 0.5),\n",
        "    \"aug_ambience_prob\": 0.8,\n",
        "    \"aug_ambience_snr\": (0, 20),\n",
        "    \"aug_dist_prob\": 0.3,\n",
        "    \"aug_dist_drive\": (0.5, 1.5),\n",
        "    \"synthetic_rir_rt60\": (0.2, 1.2),  # fallback if no IR files found\n",
        "\n",
        "    # ---- OpenL3 (TensorFlow GPU) ----\n",
        "    \"openl3_input_repr\": \"mel256\",\n",
        "    \"openl3_content\": \"music\",\n",
        "    \"openl3_embed_dim\": 512,\n",
        "    \"openl3_hop\": 0.1,\n",
        "    \"openl3_batch_pooled\": 64,\n",
        "    \"openl3_batch_seq\": 16,\n",
        "    \"openl3_batchsize_tf\": 64,   # TF batch size inside get_audio_embedding\n",
        "\n",
        "    # ---- AudioMAE (PyTorch HF) ----\n",
        "    \"audiomae_model_id\": \"facebook/audio-mae-base\",\n",
        "    \"audiomae_batch_pooled\": 32,\n",
        "    \"audiomae_batch_seq\": 16,\n",
        "\n",
        "    # ---- Training (PyTorch heads) ----\n",
        "    \"epochs\": 25,\n",
        "    \"lr\": 3e-4,\n",
        "    \"weight_decay\": 0.05,\n",
        "    \"width_mlp\": 1024,\n",
        "    \"dropout\": 0.2,\n",
        "\n",
        "    # ---- Real IR / Ambience URLs ----\n",
        "    \"ir_urls\": [\n",
        "        \"https://mcdermottlab.mit.edu/Reverb/IRMAudio/Audio.zip\",\n",
        "        \"https://webfiles.york.ac.uk/OPENAIR/IRs/tvisongur-sound-sculpture-iceland-model/stereo/source1domefareceiver2domelabinaural.wav\"\n",
        "\n",
        "    ],\n",
        "    \"amb_urls\": [\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/7/76/Ambience_noise_at_1_am_in_Bengaluru%2C_Karnataka%2C_India.wav\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/5/54/Cafe_ambiance.ogg\",\n",
        "    ],\n",
        "}\n"
      ],
      "metadata": {
        "id": "_5_SNOcIcO9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETUP\n"
      ],
      "metadata": {
        "id": "ptrx9aH-G6k9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "get the openl3 model\n"
      ],
      "metadata": {
        "id": "cq9cB10rAZCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "clone the openl3 repo, this can be much easier with pip if we use pip and a custom python environment, but on colab we directly use the repo due to compatability issues."
      ],
      "metadata": {
        "id": "4S66RPadAv65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/marl/openl3.git /content/openl3\n"
      ],
      "metadata": {
        "id": "hzWKz-Te_Q5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "install dependencies that colab does not come with."
      ],
      "metadata": {
        "id": "L7kLGRVfBDNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#openl3 dependants\n",
        "!pip install resampy\n",
        "!pip install kapre"
      ],
      "metadata": {
        "id": "u_10C6vTAf3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set up imports and tensorflow"
      ],
      "metadata": {
        "id": "RN07GuzVBR1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Imports & paths\n",
        "import os, glob, zipfile, subprocess, shutil, random, sys, gzip, shutil, urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np, pandas as pd, librosa, soundfile as sf, matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import scipy.signal as sig\n",
        "\n",
        "import math, threading\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# PyTorch for heads & AudioMAE\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import AutoFeatureExtractor, AutoModel\n",
        "\n",
        "# insert the onpenl3 path\n",
        "REPO = \"/content/openl3\"\n",
        "assert os.path.isdir(REPO) and os.path.isdir(os.path.join(REPO, \"openl3\"))\n",
        "if REPO not in sys.path:\n",
        "    sys.path.insert(0, REPO)  # repo takes precedence\n",
        "\n",
        "# TensorFlow GPU for openl3 embeddings\n",
        "import tensorflow as tf, openl3\n",
        "\n",
        "# ---- TF GPU config (memory growth to avoid OOM) ----\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "for g in gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    except Exception:\n",
        "        pass\n",
        "print(\"TF GPUs:\", gpus)\n",
        "\n",
        "SEED = hyper[\"seed\"]\n",
        "np.random.seed(SEED); random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if (torch.cuda.is_available() and hyper[\"device\"]==\"cuda\") else \"cpu\"\n",
        "TARGET_SR = hyper[\"target_sr\"]; USE_AMP = hyper[\"use_amp\"]\n",
        "\n",
        "ROOT = Path(hyper[\"runtime_root\"]) / hyper[\"dataset_root\"]\n",
        "AUDIO_RAW = ROOT / \"audio_raw\"; AUDIO_16K = ROOT / \"audio_16k\"\n",
        "IR_DIR = ROOT / \"irs\"; AMB_DIR = ROOT / \"ambiences\"; CACHE = ROOT / \"cache\"; TMP = ROOT / \"tmp\"\n",
        "for p in [ROOT, AUDIO_RAW, AUDIO_16K, IR_DIR, AMB_DIR, CACHE, TMP]: p.mkdir(parents=True, exist_ok=True)\n",
        "print(\"PyTorch device:\", DEVICE)\n"
      ],
      "metadata": {
        "id": "06Yuru5Icbwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the openl3 weights"
      ],
      "metadata": {
        "id": "QM5uCKVaCE_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "don't need to change anything here, the weights are stored on runtime disk."
      ],
      "metadata": {
        "id": "Lma7rR_1BgBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# destination folder where openl3 looks for weights\n",
        "DEST_DIR = \"/content/openl3/openl3\"\n",
        "os.makedirs(DEST_DIR, exist_ok=True)\n",
        "\n",
        "# GitHub release where all .h5.gz weights are stored\n",
        "RELEASE_VER = \"v0_4_0\"\n",
        "BASE_URL = f\"https://github.com/marl/openl3/raw/models/\"\n",
        "\n",
        "modalities   = [\"audio\", \"image\"]\n",
        "input_reprs  = [\"linear\", \"mel128\", \"mel256\"]\n",
        "content_type = [\"music\", \"env\"]\n",
        "\n",
        "for m in modalities:\n",
        "    for r in input_reprs:\n",
        "        for c in content_type:\n",
        "            base = f\"openl3_{m}_{r}_{c}\"\n",
        "            gz_name = f\"{base}-{RELEASE_VER}.h5.gz\"\n",
        "            gz_path = os.path.join(DEST_DIR, gz_name)\n",
        "            h5_path = os.path.join(DEST_DIR, f\"{base}.h5\")\n",
        "\n",
        "            if os.path.exists(h5_path):\n",
        "                print(f\"✓ {os.path.basename(h5_path)} already exists\")\n",
        "                continue\n",
        "\n",
        "            url = BASE_URL + gz_name\n",
        "            print(url)\n",
        "            print(f\"↓ Downloading {gz_name} ...\")\n",
        "            urllib.request.urlretrieve(url, gz_path)\n",
        "\n",
        "            print(f\"• Decompressing {gz_name} ...\")\n",
        "            with gzip.open(gz_path, \"rb\") as src, open(h5_path, \"wb\") as dst:\n",
        "                shutil.copyfileobj(src, dst)\n",
        "\n",
        "            os.remove(gz_path)\n",
        "            print(f\"✓ Saved {os.path.basename(h5_path)}\\n\")\n",
        "\n",
        "print(\"✅ All OpenL3 weight files are downloaded to:\", DEST_DIR)"
      ],
      "metadata": {
        "id": "44BwP5QxCCrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "debug\n"
      ],
      "metadata": {
        "id": "c_qu1y4LCSGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "simple block to test if openl3 is up"
      ],
      "metadata": {
        "id": "Z0lNCFLtBxlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF:\", tf.__version__, \"| GPUs:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Load pretrained OpenL3\n",
        "l3 = openl3.models.load_audio_embedding_model(\"mel256\", \"music\", 512)\n",
        "\n",
        "# 1s sine @16 kHz\n",
        "sr = 16000\n",
        "t = np.linspace(0, 1, sr, endpoint=False, dtype=np.float32)\n",
        "y = 0.2*np.sin(2*np.pi*440*t).astype(np.float32)\n",
        "\n",
        "E, ts = openl3.core.get_audio_embedding(y, sr, model=l3, hop_size=0.1, center=True, batch_size=64)\n",
        "print(\"Embeddings:\", E.shape, \"frames:\", len(ts))"
      ],
      "metadata": {
        "id": "JAX6hq-XCQVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOWNLOAD DATA"
      ],
      "metadata": {
        "id": "nil6N7AhIPfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual Data"
      ],
      "metadata": {
        "id": "P47JycEbZFcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "dzEGeVspuptA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for reverb"
      ],
      "metadata": {
        "id": "r1YuxUX1uqvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "block that processes the dataset from previous pipelines, so team mates uploaded audio files in folders and a csv of these clips. This block is to decompress, extract, and form a dataset at runtime. Highly specialized to the certain file structure and naming we are using for the medleydb set and the data structure of medleydb, will not work smoothly with other datastructure. PLease refer to data augmentation pielines for how they specifcally output files."
      ],
      "metadata": {
        "id": "WvD8hb-4CF-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 1) Paths (Google Drive)\n",
        "# =========================\n",
        "# Folder in your Drive that contains:\n",
        "#   - audio_1000_balanced.zip\n",
        "#   - audio_1000_balanced_Reverb1.zip ... Reverb10.zip\n",
        "#   - audio_labels_1000_balanced.csv\n",
        "#   - reverb_csv_1-10.zip  (or similar)\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/medley_db_enhanced\")\n",
        "\n",
        "# We'll extract audio next to the zips (into subfolders)\n",
        "OUT_AUDIO = DATA_ROOT   # so /medley_db_enhanced/audio_1000_balanced*, etc.\n",
        "\n",
        "BASE_ZIP = DATA_ROOT / \"audio_1000_balanced.zip\"\n",
        "REVERB_ZIPS = [DATA_ROOT / f\"audio_1000_balanced_Reverb{i}.zip\" for i in range(1, 11)]\n",
        "\n",
        "BASE_CSV   = DATA_ROOT / \"audio_labels_1000_balanced.csv\"\n",
        "REVERB_CSV = DATA_ROOT / \"reverb_csv_1-10.zip\"   # or a single CSV if that's what you have\n",
        "\n",
        "AUDIO_EXTS = (\".wav\", \".aiff\", \".aif\", \".flac\", \".ogg\", \".mp3\", \".m4a\")\n",
        "\n",
        "# =========================\n",
        "# 2) Unzip audio into folders\n",
        "#    /medley_db_enhanced/audio_1000_balanced\n",
        "#    /medley_db_enhanced/audio_1000_balanced_Reverb1\n",
        "#    ...\n",
        "# =========================\n",
        "def unzip_to(zip_path: Path, out_dir: Path):\n",
        "    \"\"\"Extract only audio files to a subfolder named after the zip stem.\"\"\"\n",
        "    if not zip_path.exists():\n",
        "        print(\"Missing:\", zip_path)\n",
        "        return\n",
        "    sub = out_dir / zip_path.stem\n",
        "    sub.mkdir(parents=True, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        members = [m for m in zf.namelist() if m.lower().endswith(AUDIO_EXTS)]\n",
        "        for m in tqdm(members, desc=f\"Extract → {zip_path.stem}\"):\n",
        "            tgt = sub / Path(m).name\n",
        "            if not tgt.exists():\n",
        "                with zf.open(m) as src, open(tgt, \"wb\") as dst:\n",
        "                    dst.write(src.read())\n",
        "\n",
        "# Unpack base + all reverbs\n",
        "unzip_to(BASE_ZIP, OUT_AUDIO)\n",
        "for z in REVERB_ZIPS:\n",
        "    unzip_to(z, OUT_AUDIO)\n",
        "\n",
        "# =========================\n",
        "# 3) Robust CSV reader for reverb CSVs\n",
        "# =========================\n",
        "def read_reverb_csvs(zip_or_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"Read one ZIP of CSVs or a single CSV, with encoding fallback.\"\"\"\n",
        "    def read_one(fobj):\n",
        "        try:\n",
        "            return pd.read_csv(fobj)\n",
        "        except UnicodeDecodeError:\n",
        "            fobj.seek(0)\n",
        "            return pd.read_csv(fobj, encoding=\"latin1\")\n",
        "\n",
        "    if zip_or_csv.suffix.lower() == \".zip\":\n",
        "        with zipfile.ZipFile(zip_or_csv, \"r\") as zf:\n",
        "            csv_names = [n for n in zf.namelist() if n.lower().endswith(\".csv\")]\n",
        "            frames = []\n",
        "            for name in csv_names:\n",
        "                with zf.open(name) as f:\n",
        "                    frames.append(read_one(f))\n",
        "            return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
        "    else:\n",
        "        try:\n",
        "            return pd.read_csv(zip_or_csv)\n",
        "        except UnicodeDecodeError:\n",
        "            return pd.read_csv(zip_or_csv, encoding=\"latin1\")\n",
        "\n",
        "# Base (dry) CSV\n",
        "base_df = pd.read_csv(BASE_CSV)\n",
        "base_df[\"reverb_label\"] = \"Dry\"\n",
        "\n",
        "# Reverb CSV(s)\n",
        "rev_df = read_reverb_csvs(REVERB_CSV)\n",
        "\n",
        "# =========================\n",
        "# 4) Merge + normalize column names\n",
        "# =========================\n",
        "df_all = pd.concat([base_df, rev_df], ignore_index=True)\n",
        "df_all = df_all.rename(columns={c: c.strip().lower() for c in df_all.columns})\n",
        "\n",
        "# If your uuid column isn't exactly \"uuid4\", auto-rename it:\n",
        "if \"uuid4\" not in df_all.columns:\n",
        "    uuid_candidates = [c for c in df_all.columns if \"uuid\" in c.lower()]\n",
        "    if uuid_candidates:\n",
        "        df_all = df_all.rename(columns={uuid_candidates[0]: \"uuid4\"})\n",
        "\n",
        "# Same for reverb_label, if needed:\n",
        "if \"reverb_label\" not in df_all.columns:\n",
        "    rev_candidates = [c for c in df_all.columns if \"reverb\" in c.lower()]\n",
        "    if rev_candidates:\n",
        "        df_all = df_all.rename(columns={rev_candidates[0]: \"reverb_label\"})\n",
        "\n",
        "# =========================\n",
        "# 5) Resolve audio paths by uuid4 + reverb_label\n",
        "# =========================\n",
        "def find_path_for_uuid(uuid: str, reverb_label: str | None) -> str | None:\n",
        "    uuid = str(uuid).lower()\n",
        "\n",
        "    # First: try the specific reverb folder, e.g. audio_1000_balanced_Reverb4\n",
        "    if reverb_label and reverb_label != \"Dry\":\n",
        "        pref = OUT_AUDIO / f\"audio_1000_balanced_{reverb_label}\"\n",
        "        if pref.exists():\n",
        "            for p in pref.iterdir():\n",
        "                if p.suffix.lower() in AUDIO_EXTS and uuid in p.stem.lower():\n",
        "                    return p.as_posix()\n",
        "\n",
        "    # Dry or fallback: search anywhere under OUT_AUDIO\n",
        "    for p in OUT_AUDIO.rglob(\"*\"):\n",
        "        if p.suffix.lower() in AUDIO_EXTS and uuid in p.stem.lower():\n",
        "            return p.as_posix()\n",
        "\n",
        "    return None\n",
        "\n",
        "paths = []\n",
        "for r in tqdm(df_all.to_dict(\"records\"), desc=\"Resolve audio paths\"):\n",
        "    paths.append(find_path_for_uuid(r.get(\"uuid4\", \"\"), r.get(\"reverb_label\")))\n",
        "df_all[\"path\"] = paths\n",
        "df_all = df_all.dropna(subset=[\"path\"]).reset_index(drop=True)\n",
        "\n",
        "# Encode instrument labels into integers (y)\n",
        "df_all[\"y\"] = df_all[\"instrument\"].astype(\"category\").cat.codes\n",
        "classes = list(df_all[\"instrument\"].astype(\"category\").cat.categories)\n",
        "\n",
        "# Expose as `df` so later blocks that expect `df` still work\n",
        "df = df_all\n",
        "\n",
        "print(\"Rows:\", len(df))\n",
        "print(\"Instruments:\", classes)\n",
        "print(df[[\"subset\", \"instrument\", \"reverb_label\", \"uuid4\", \"path\"]].head(5))\n"
      ],
      "metadata": {
        "id": "vAnKOmMpZCeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "noise"
      ],
      "metadata": {
        "id": "MpcbbMZfxI29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the same block as the upper one only modified for the noise augmented set. The noise set presents data in a similar file structure as the reverb set."
      ],
      "metadata": {
        "id": "pcUwvUibC0Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 1) Paths (Google Drive)\n",
        "# =========================\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/medley_db_enhanced\")\n",
        "\n",
        "# 10 noise zips:\n",
        "NOISE_ZIPS = [DATA_ROOT / f\"augmentation_noise_part_{i:02d}.zip\"\n",
        "              for i in range(1, 11)]\n",
        "\n",
        "# single labels CSV for the 10k noisy clips\n",
        "NOISE_CSV = DATA_ROOT / \"augmented_audio_labels_10000.csv\"\n",
        "\n",
        "# where to extract noisy audio\n",
        "NOISE_AUDIO_ROOT = DATA_ROOT / \"augmentation_noise_audio\"\n",
        "NOISE_AUDIO_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "AUDIO_EXTS = (\".wav\", \".aiff\", \".aif\", \".flac\", \".ogg\", \".mp3\", \".m4a\")\n",
        "\n",
        "# =========================\n",
        "# 2) Unzip noise audio\n",
        "#    /.../augmentation_noise_audio/augmentation_noise_part_01\n",
        "#    ...\n",
        "# =========================\n",
        "def unzip_to(zip_path: Path, out_dir: Path):\n",
        "    \"\"\"Extract only audio files to a subfolder named after the zip stem.\"\"\"\n",
        "    if not zip_path.exists():\n",
        "        print(\"Missing:\", zip_path)\n",
        "        return\n",
        "    sub = out_dir / zip_path.stem\n",
        "    sub.mkdir(parents=True, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        members = [m for m in zf.namelist()\n",
        "                   if m.lower().endswith(AUDIO_EXTS)]\n",
        "        for m in tqdm(members, desc=f\"Extract → {zip_path.stem}\"):\n",
        "            tgt = sub / Path(m).name\n",
        "            if not tgt.exists():\n",
        "                with zf.open(m) as src, open(tgt, \"wb\") as dst:\n",
        "                    dst.write(src.read())\n",
        "\n",
        "for z in NOISE_ZIPS:\n",
        "    unzip_to(z, NOISE_AUDIO_ROOT)\n",
        "\n",
        "# =========================\n",
        "# 3) Read labels CSV\n",
        "# =========================\n",
        "def read_noise_csv(csv_path: Path) -> pd.DataFrame:\n",
        "    try:\n",
        "        return pd.read_csv(csv_path)\n",
        "    except UnicodeDecodeError:\n",
        "        return pd.read_csv(csv_path, encoding=\"latin1\")\n",
        "\n",
        "df_noise = read_noise_csv(NOISE_CSV)\n",
        "\n",
        "# normalize column names\n",
        "df_noise = df_noise.rename(columns={c: c.strip().lower()\n",
        "                                    for c in df_noise.columns})\n",
        "\n",
        "# ensure we have \"uuid4\" and \"noise_category\" column names\n",
        "if \"uuid4\" not in df_noise.columns:\n",
        "    uuid_candidates = [c for c in df_noise.columns if \"uuid\" in c.lower()]\n",
        "    if uuid_candidates:\n",
        "        df_noise = df_noise.rename(columns={uuid_candidates[0]: \"uuid4\"})\n",
        "\n",
        "if \"noise_category\" not in df_noise.columns:\n",
        "    noise_candidates = [c for c in df_noise.columns if \"noise\" in c.lower()]\n",
        "    if noise_candidates:\n",
        "        df_noise = df_noise.rename(columns={noise_candidates[0]: \"noise_category\"})\n",
        "\n",
        "# =========================\n",
        "# 4) Resolve audio paths by uuid4 + noise_category\n",
        "# =========================\n",
        "def find_noise_path(uuid: str, noise_category: str | None) -> str | None:\n",
        "    \"\"\"\n",
        "    Search inside NOISE_AUDIO_ROOT for a file whose name contains the uuid,\n",
        "    and (if available) the noise_category string.\n",
        "    \"\"\"\n",
        "    uuid = str(uuid).lower()\n",
        "    noise_category = (str(noise_category).lower()\n",
        "                      if noise_category is not None else None)\n",
        "\n",
        "    for p in NOISE_AUDIO_ROOT.rglob(\"*\"):\n",
        "        if p.suffix.lower() not in AUDIO_EXTS:\n",
        "            continue\n",
        "        name = p.stem.lower()\n",
        "        if uuid in name:\n",
        "            if (noise_category is None) or (noise_category in name):\n",
        "                return p.as_posix()\n",
        "    return None\n",
        "\n",
        "paths = []\n",
        "for r in tqdm(df_noise.to_dict(\"records\"),\n",
        "              desc=\"Resolve noise-augmented audio paths\"):\n",
        "    paths.append(find_noise_path(r.get(\"uuid4\", \"\"),\n",
        "                                 r.get(\"noise_category\")))\n",
        "\n",
        "df_noise[\"path\"] = paths\n",
        "df_noise = df_noise.dropna(subset=[\"path\"]).reset_index(drop=True)\n",
        "\n",
        "# Encode instruments into integer labels (like before)\n",
        "df_noise[\"y\"] = df_noise[\"instrument\"].astype(\"category\").cat.codes\n",
        "noise_classes = list(df_noise[\"instrument\"].astype(\"category\").cat.categories)\n",
        "\n",
        "# Optionally expose as `df` so the embedding code can reuse it directly\n",
        "df = df_noise\n",
        "\n",
        "print(\"Noise rows:\", len(df_noise))\n",
        "print(\"Instruments:\", noise_classes)\n",
        "print(df_noise[[\"subset\", \"instrument\",\n",
        "                \"noise_category\", \"uuid4\", \"path\"]].head(10))\n"
      ],
      "metadata": {
        "id": "Gjpn22yUxHxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split dataset"
      ],
      "metadata": {
        "id": "0uHpEFmgdDu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "manual set data split"
      ],
      "metadata": {
        "id": "VBPFUn49ZPDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "not really needed, can skip since we are not training."
      ],
      "metadata": {
        "id": "wwJcFrgRDaID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= Splits (your loaders look for subset strings) =======\n",
        "df = df_all.copy()\n",
        "\n",
        "train_df = df[df[\"subset\"].str.contains(\"train\", case=False, na=False)].reset_index(drop=True)\n",
        "val_df   = df[df[\"subset\"].str.contains(\"validation\", case=False, na=False)].reset_index(drop=True)\n",
        "test_df  = df[df[\"subset\"].str.contains(\"test\", case=False, na=False)].reset_index(drop=True)\n",
        "\n",
        "print(\"Counts:\", len(train_df), len(val_df), len(test_df))\n",
        "print(\"Reverbs in train:\", train_df[\"reverb_label\"].value_counts().to_dict())\n"
      ],
      "metadata": {
        "id": "8rNkB-hlZRxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding inference"
      ],
      "metadata": {
        "id": "0YvFsiXucS_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading one community hf audiomae model on the fly and getting embeddings for the reverb set for both openl3 and audiomae. Still, highly specialized for the file structure we are using, change the folder address as needed. Also, tqdm printouts will slow the browser. This block will output embeddings in .npy files for each audio clip, and one csv will be also provided for the embeddings. Audiomae will clip at 10s for audio clips, not a pipeline bug but a model design, need additional tweaking if intended to process data above 10s."
      ],
      "metadata": {
        "id": "Gd_VZRFADpFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Inference-only embeddings for ALL tracks (OpenL3 + AudioMAE) ======\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel\n",
        "from openl3 import models as o3models, core as o3core\n",
        "\n",
        "# If you're in a fresh Colab session, re-mount Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ---------- 0) Paths (Google Drive) ----------\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/medley_db_enhanced\")\n",
        "\n",
        "EMB_ROOT   = DATA_ROOT / \"embeds\"\n",
        "EMB_L3_DIR = EMB_ROOT / \"openl3_mel256_music_512\"\n",
        "EMB_AM_DIR = EMB_ROOT / \"audiomae_base_768\"\n",
        "EMB_L3_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EMB_AM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- 1) Use the EXISTING df with resolved paths ----------\n",
        "# Make sure you've already run the earlier block that builds `df`\n",
        "# with columns: [\"uuid4\", \"instrument\", \"reverb_label\", \"path\", ...]\n",
        "required_cols = {\"uuid4\", \"instrument\", \"reverb_label\", \"path\"}\n",
        "assert required_cols.issubset(df.columns), f\"df is missing required columns: {required_cols - set(df.columns)}\"\n",
        "\n",
        "print(\"Rows in df:\", len(df))\n",
        "print(df[[\"subset\", \"instrument\", \"reverb_label\", \"uuid4\", \"path\"]].head())\n",
        "\n",
        "# ---------- 2) OpenL3 model & embed function ----------\n",
        "L3_SR = 48000\n",
        "l3_model = o3models.load_audio_embedding_model(\"mel256\", \"music\", 512)\n",
        "\n",
        "def openl3_embed_file(path: str, target_sr=L3_SR) -> np.ndarray:\n",
        "    \"\"\"Return pooled OpenL3 embedding [512].\"\"\"\n",
        "    y, sr = librosa.load(path, sr=target_sr, mono=True)\n",
        "    E, ts = o3core.get_audio_embedding(\n",
        "        y, target_sr,\n",
        "        model=l3_model,\n",
        "        hop_size=0.5,\n",
        "        center=True,\n",
        "        batch_size=64\n",
        "    )\n",
        "    return E.mean(axis=0).astype(np.float32)  # [512]\n",
        "\n",
        "# ---------- 3) AudioMAE (hance-ai) & embed function ----------\n",
        "mae_id = \"hance-ai/audiomae\"\n",
        "mae_cfg = AutoConfig.from_pretrained(mae_id, trust_remote_code=True)\n",
        "mae     = AutoModel.from_pretrained(mae_id, trust_remote_code=True).to(DEVICE).eval()\n",
        "\n",
        "def audiomae_embed_file(path: str) -> np.ndarray:\n",
        "    \"\"\"Return pooled AudioMAE embedding, e.g. [768].\"\"\"\n",
        "    with torch.no_grad():\n",
        "        Z = mae(path)  # hance-ai implementation: returns (D, H', W')\n",
        "        if isinstance(Z, (tuple, list)):\n",
        "            Z = Z[0]\n",
        "        if Z.ndim == 3:      # (D, H, W)\n",
        "            v = Z.mean(dim=(1, 2)).float().cpu().numpy()\n",
        "        elif Z.ndim == 2:    # (T, D) – just in case\n",
        "            v = Z.mean(dim=0).float().cpu().numpy()\n",
        "        else:                # fallback\n",
        "            v = Z.flatten().float().cpu().numpy()\n",
        "    return v.astype(np.float32)\n",
        "\n",
        "# ---------- 4) Helpers to save vectors ----------\n",
        "def safe_name(s: str) -> str:\n",
        "    import re\n",
        "    return re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", str(s))\n",
        "\n",
        "def save_vec(vec: np.ndarray, out_dir: Path, reverb_label: str, uuid: str) -> Path:\n",
        "    d = out_dir / safe_name(reverb_label)\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "    outp = d / f\"{uuid}.npy\"\n",
        "    np.save(outp.as_posix(), vec)\n",
        "    return outp\n",
        "\n",
        "# ---------- 5) Run embeddings over ALL rows ----------\n",
        "l3_outs, mae_outs = [], []\n",
        "\n",
        "for r in tqdm(df.to_dict(\"records\"), desc=\"Embedding all\"):\n",
        "    p    = r[\"path\"]\n",
        "    uuid = str(r[\"uuid4\"])\n",
        "    rev  = str(r[\"reverb_label\"])\n",
        "\n",
        "    # OpenL3\n",
        "    try:\n",
        "        v = openl3_embed_file(p)\n",
        "        l3_path = save_vec(v, EMB_L3_DIR, rev, uuid)\n",
        "        l3_outs.append(l3_path.as_posix())\n",
        "    except Exception as e:\n",
        "        print(\"OpenL3 failed on\", p, \"->\", e)\n",
        "        l3_outs.append(None)\n",
        "\n",
        "    # AudioMAE\n",
        "    try:\n",
        "        v = audiomae_embed_file(p)\n",
        "        am_path = save_vec(v, EMB_AM_DIR, rev, uuid)\n",
        "        mae_outs.append(am_path.as_posix())\n",
        "    except Exception as e:\n",
        "        print(\"AudioMAE failed on\", p, \"->\", e)\n",
        "        mae_outs.append(None)\n",
        "\n",
        "df[\"embed_openl3\"]   = l3_outs\n",
        "df[\"embed_audiomae\"] = mae_outs\n",
        "\n",
        "# Drop rows where one of the embeddings failed\n",
        "df = df.dropna(subset=[\"embed_openl3\", \"embed_audiomae\"]).reset_index(drop=True)\n",
        "\n",
        "manifest_path = DATA_ROOT / \"balanced_all_embeddings_manifest.csv\"\n",
        "df.to_csv(manifest_path, index=False)\n",
        "print(\"✅ Done. Manifest:\", manifest_path.as_posix())\n",
        "print(df[[\"instrument\", \"reverb_label\", \"uuid4\", \"embed_openl3\", \"embed_audiomae\"]].head())\n"
      ],
      "metadata": {
        "id": "sFbZP7BgcRho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "noise"
      ],
      "metadata": {
        "id": "jaB-UHCvyXVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "same thing as the above, but directed to the noise set."
      ],
      "metadata": {
        "id": "cu9dERyzESjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Inference-only embeddings for ALL NOISE-AUGMENTED tracks ======\n",
        "\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModel\n",
        "from openl3 import models as o3models, core as o3core\n",
        "\n",
        "# (Re-)mount Drive if needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Make sure torchcodec is installed for AudioMAE / torchaudio ---\n",
        "try:\n",
        "    import torchcodec  # noqa: F401\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    print(\"Installing torchcodec (required by torchaudio/load_with_torchcodec)...\")\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"torchcodec\", \"-q\"],\n",
        "        check=True\n",
        "    )\n",
        "    import torchcodec  # noqa: F401\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# ---------- 0) Paths (Google Drive) ----------\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/medley_db_enhanced\")\n",
        "\n",
        "# keep noise embeddings in their own folder so they don't mix with reverb ones\n",
        "EMB_ROOT   = DATA_ROOT / \"embeds_noise\"\n",
        "EMB_L3_DIR = EMB_ROOT / \"openl3_mel256_music_512\"\n",
        "EMB_AM_DIR = EMB_ROOT / \"audiomae_base_768\"\n",
        "EMB_L3_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EMB_AM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- 1) Use the EXISTING df from the noise loader ----------\n",
        "# That loader should have produced df with:\n",
        "# [\"subset\", \"instrument\", \"noise_category\", \"uuid4\", \"path\", \"y\", ...]\n",
        "required_cols = {\"uuid4\", \"instrument\", \"noise_category\", \"path\"}\n",
        "assert required_cols.issubset(df.columns), \\\n",
        "    f\"df is missing required columns: {required_cols - set(df.columns)}\"\n",
        "\n",
        "print(\"Noise rows in df:\", len(df))\n",
        "print(df[[\"subset\", \"instrument\", \"noise_category\", \"uuid4\", \"path\"]].head())\n",
        "\n",
        "# ---------- 2) OpenL3 model & embed function ----------\n",
        "L3_SR = 48000\n",
        "l3_model = o3models.load_audio_embedding_model(\"mel256\", \"music\", 512)\n",
        "\n",
        "def openl3_embed_file(path: str, target_sr=L3_SR) -> np.ndarray:\n",
        "    \"\"\"Return pooled OpenL3 embedding [512].\"\"\"\n",
        "    y, sr = librosa.load(path, sr=target_sr, mono=True)\n",
        "    E, ts = o3core.get_audio_embedding(\n",
        "        y, target_sr,\n",
        "        model=l3_model,\n",
        "        hop_size=0.5,\n",
        "        center=True,\n",
        "        batch_size=64\n",
        "    )\n",
        "    return E.mean(axis=0).astype(np.float32)  # [512]\n",
        "\n",
        "# ---------- 3) AudioMAE (hance-ai) & embed function ----------\n",
        "mae_id = \"hance-ai/audiomae\"\n",
        "mae_cfg = AutoConfig.from_pretrained(mae_id, trust_remote_code=True)\n",
        "mae     = AutoModel.from_pretrained(mae_id, trust_remote_code=True).to(DEVICE).eval()\n",
        "\n",
        "def audiomae_embed_file(path: str) -> np.ndarray:\n",
        "    \"\"\"Return pooled AudioMAE embedding, e.g. [768].\"\"\"\n",
        "    with torch.no_grad():\n",
        "        Z = mae(path)  # hance-ai implementation: returns (D, H', W')\n",
        "        if isinstance(Z, (tuple, list)):\n",
        "            Z = Z[0]\n",
        "        if Z.ndim == 3:      # (D, H, W)\n",
        "            v = Z.mean(dim=(1, 2)).float().cpu().numpy()\n",
        "        elif Z.ndim == 2:    # (T, D) – just in case\n",
        "            v = Z.mean(dim=0).float().cpu().numpy()\n",
        "        else:                # fallback\n",
        "            v = Z.flatten().float().cpu().numpy()\n",
        "    return v.astype(np.float32)\n",
        "\n",
        "# ---------- 4) Helpers to save vectors ----------\n",
        "def safe_name(s: str) -> str:\n",
        "    import re\n",
        "    return re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", str(s))\n",
        "\n",
        "def save_vec(vec: np.ndarray, out_dir: Path, noise_category: str, uuid: str) -> Path:\n",
        "    \"\"\"\n",
        "    Save one vector as:\n",
        "        {out_dir}/{noise_category}/{uuid}.npy\n",
        "    e.g. embeds_noise/openl3_mel256_music_512/ambience/005d3d9e-....npy\n",
        "    \"\"\"\n",
        "    d = out_dir / safe_name(noise_category)\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "    outp = d / f\"{uuid}.npy\"\n",
        "    np.save(outp.as_posix(), vec)\n",
        "    return outp\n",
        "\n",
        "# ---------- 5) Run embeddings over ALL rows ----------\n",
        "l3_outs, mae_outs = [], []\n",
        "\n",
        "for r in tqdm(df.to_dict(\"records\"), desc=\"Embedding all (noise set)\"):\n",
        "    p     = r[\"path\"]\n",
        "    uuid  = str(r[\"uuid4\"])\n",
        "    noise = str(r[\"noise_category\"])\n",
        "\n",
        "    # OpenL3\n",
        "    try:\n",
        "        v = openl3_embed_file(p)\n",
        "        l3_path = save_vec(v, EMB_L3_DIR, noise, uuid)\n",
        "        l3_outs.append(l3_path.as_posix())\n",
        "    except Exception as e:\n",
        "        print(\"OpenL3 failed on\", p, \"->\", e)\n",
        "        l3_outs.append(None)\n",
        "\n",
        "    # AudioMAE\n",
        "    try:\n",
        "        v = audiomae_embed_file(p)\n",
        "        am_path = save_vec(v, EMB_AM_DIR, noise, uuid)\n",
        "        mae_outs.append(am_path.as_posix())\n",
        "    except Exception as e:\n",
        "        print(\"AudioMAE failed on\", p, \"->\", e)\n",
        "        mae_outs.append(None)\n",
        "\n",
        "df[\"embed_openl3_noise\"]   = l3_outs\n",
        "df[\"embed_audiomae_noise\"] = mae_outs\n",
        "\n",
        "# Drop rows where one of the embeddings failed\n",
        "df = df.dropna(subset=[\"embed_openl3_noise\", \"embed_audiomae_noise\"]).reset_index(drop=True)\n",
        "\n",
        "manifest_path = DATA_ROOT / \"noise_embeddings_manifest.csv\"\n",
        "df.to_csv(manifest_path, index=False)\n",
        "print(\"✅ Done. Manifest:\", manifest_path.as_posix())\n",
        "print(df[[\"instrument\", \"noise_category\", \"uuid4\",\n",
        "          \"embed_openl3_noise\", \"embed_audiomae_noise\"]].head())\n"
      ],
      "metadata": {
        "id": "CMrUgi4HyVvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checks"
      ],
      "metadata": {
        "id": "atLQDRyKKPis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "below are some simple checks to see the quality of the extracted embeddings. As of now only works for the rever set, will require minor tweaks to expected file names and destinations if to be used on the noise set."
      ],
      "metadata": {
        "id": "FugnEhEPEZcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = Path(\"/content/drive/MyDrive/medley_db_enhanced\")\n",
        "manifest_path = DATA_ROOT / \"noise_embeddings_manifest.csv\"\n",
        "\n",
        "df = pd.read_csv(manifest_path)\n",
        "print(\"Rows in manifest:\", len(df))\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "03RBdbYWKRG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def check_model_column(col_name, expected_dim=None, max_checks=200):\n",
        "    print(f\"\\n=== Checking {col_name} ===\")\n",
        "    paths = df[col_name].dropna().tolist()\n",
        "    missing = [p for p in paths if not Path(p).exists()]\n",
        "    print(\"Total entries:\", len(paths))\n",
        "    print(\"Missing files:\", len(missing))\n",
        "    if missing:\n",
        "        print(\"First few missing:\", missing[:5])\n",
        "\n",
        "    shapes = []\n",
        "    for p in paths[:max_checks]:\n",
        "        v = np.load(p)\n",
        "        shapes.append(v.shape)\n",
        "    counts = Counter(shapes)\n",
        "    print(\"Shape counts (first\", max_checks, \"files):\")\n",
        "    for s, c in counts.items():\n",
        "        print(\"  \", s, \"->\", c, \"files\")\n",
        "    if expected_dim is not None:\n",
        "        ok = all((len(s) == 1 and s[0] == expected_dim) for s in counts.keys())\n",
        "        print(\"Matches expected dim\", expected_dim, \"?\", ok)\n",
        "    return missing, counts\n",
        "\n",
        "missing_l3, shapes_l3   = check_model_column(\"embed_openl3\",   expected_dim=512)\n",
        "missing_mae, shapes_mae = check_model_column(\"embed_audiomae\", expected_dim=768)\n"
      ],
      "metadata": {
        "id": "fNLL2SYhKST3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_uuid_l3 = []\n",
        "bad_uuid_mae = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    uuid = str(row[\"uuid4\"])\n",
        "\n",
        "    # OpenL3\n",
        "    p_l3 = row.get(\"embed_openl3\", None)\n",
        "    if isinstance(p_l3, str) and p_l3:\n",
        "        stem_l3 = Path(p_l3).stem  # filename without extension\n",
        "        if stem_l3 != uuid:\n",
        "            bad_uuid_l3.append((uuid, stem_l3, p_l3))\n",
        "\n",
        "    # AudioMAE\n",
        "    p_mae = row.get(\"embed_audiomae\", None)\n",
        "    if isinstance(p_mae, str) and p_mae:\n",
        "        stem_mae = Path(p_mae).stem\n",
        "        if stem_mae != uuid:\n",
        "            bad_uuid_mae.append((uuid, stem_mae, p_mae))\n",
        "\n",
        "print(\"\\nBad UUIDs (OpenL3):\", len(bad_uuid_l3))\n",
        "if bad_uuid_l3:\n",
        "    print(\"First few:\", bad_uuid_l3[:5])\n",
        "\n",
        "print(\"Bad UUIDs (AudioMAE):\", len(bad_uuid_mae))\n",
        "if bad_uuid_mae:\n",
        "    print(\"First few:\", bad_uuid_mae[:5])\n"
      ],
      "metadata": {
        "id": "Qqjv9CdIKU3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_reverb_l3 = []\n",
        "bad_reverb_mae = []\n",
        "\n",
        "def folder_label_from_path(p: str):\n",
        "    p = Path(p)\n",
        "    # parent folder under the model dir should be the reverb label (sanitized)\n",
        "    return p.parent.name\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    rev = str(row[\"reverb_label\"])\n",
        "\n",
        "    p_l3 = row.get(\"embed_openl3\", None)\n",
        "    if isinstance(p_l3, str) and p_l3:\n",
        "        folder = folder_label_from_path(p_l3)\n",
        "        if folder.lower() != rev.lower().replace(\" \", \"_\"):\n",
        "            bad_reverb_l3.append((rev, folder, p_l3))\n",
        "\n",
        "    p_mae = row.get(\"embed_audiomae\", None)\n",
        "    if isinstance(p_mae, str) and p_mae:\n",
        "        folder = folder_label_from_path(p_mae)\n",
        "        if folder.lower() != rev.lower().replace(\" \", \"_\"):\n",
        "            bad_reverb_mae.append((rev, folder, p_mae))\n",
        "\n",
        "print(\"\\nBad reverb folder matches (OpenL3):\", len(bad_reverb_l3))\n",
        "if bad_reverb_l3:\n",
        "    print(\"First few:\", bad_reverb_l3[:5])\n",
        "\n",
        "print(\"Bad reverb folder matches (AudioMAE):\", len(bad_reverb_mae))\n",
        "if bad_reverb_mae:\n",
        "    print(\"First few:\", bad_reverb_mae[:5])\n"
      ],
      "metadata": {
        "id": "zuYUQ7tKKWvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# sample a subset for speed\n",
        "MAX_POINTS = 2000\n",
        "indices = list(range(len(df)))\n",
        "random.shuffle(indices)\n",
        "indices = indices[:MAX_POINTS]\n",
        "\n",
        "# load embeddings\n",
        "X_l3 = []\n",
        "inst_labels = []\n",
        "rev_labels  = []\n",
        "\n",
        "for i in indices:\n",
        "    row = df.iloc[i]\n",
        "    v = np.load(row[\"embed_openl3\"])\n",
        "    X_l3.append(v)\n",
        "    inst_labels.append(row[\"instrument\"])\n",
        "    rev_labels.append(row[\"reverb_label\"])\n",
        "\n",
        "X_l3 = np.stack(X_l3, axis=0)  # [N, 512]\n",
        "\n",
        "# PCA to 2D\n",
        "pca = PCA(n_components=2)\n",
        "X2 = pca.fit_transform(X_l3)\n",
        "\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# ---- Color by instrument ----\n",
        "plt.figure(figsize=(8,6))\n",
        "uniq_inst = sorted(set(inst_labels))\n",
        "color_map = {inst: i for i, inst in enumerate(uniq_inst)}\n",
        "colors = [color_map[i] for i in inst_labels]\n",
        "\n",
        "sc = plt.scatter(X2[:,0], X2[:,1], c=colors, s=8, alpha=0.7, cmap=\"tab20\")\n",
        "plt.title(\"OpenL3 embeddings (PCA) – colored by instrument\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "cb = plt.colorbar(sc, ticks=range(len(uniq_inst)))\n",
        "cb.ax.set_yticklabels(uniq_inst)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zRD_EesmKYrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "import numpy as np\n",
        "\n",
        "def cosine(a, b):\n",
        "    return float(np.dot(a, b) / (norm(a)*norm(b) + 1e-9))\n",
        "\n",
        "same_uuid_cos = []\n",
        "random_cos    = []\n",
        "\n",
        "grouped = df.groupby(\"uuid4\")\n",
        "\n",
        "uuids = [u for u,g in grouped if len(g) > 1]  # multiple versions (dry + reverbs)\n",
        "print(\"UUIDs with multiple versions:\", len(uuids))\n",
        "\n",
        "# --- same-uuid pairs (dry vs some reverb, or reverb vs reverb) ---\n",
        "for uuid in np.random.choice(uuids, size=min(100, len(uuids)), replace=False):\n",
        "    g = grouped.get_group(uuid)\n",
        "    if len(g) < 2:\n",
        "        continue\n",
        "    rows = g.sample(2, replace=False)\n",
        "    v1 = np.load(rows.iloc[0][\"embed_openl3\"])\n",
        "    v2 = np.load(rows.iloc[1][\"embed_openl3\"])\n",
        "    same_uuid_cos.append(cosine(v1, v2))\n",
        "\n",
        "# --- random pairs across different uuids ---\n",
        "for _ in range(len(same_uuid_cos)):\n",
        "    pair = df.sample(2, replace=False)\n",
        "    r1 = pair.iloc[0]\n",
        "    r2 = pair.iloc[1]\n",
        "    v1 = np.load(r1[\"embed_openl3\"])\n",
        "    v2 = np.load(r2[\"embed_openl3\"])\n",
        "    random_cos.append(cosine(v1, v2))\n",
        "\n",
        "print(\"Mean cos (same uuid):\", np.mean(same_uuid_cos))\n",
        "print(\"Mean cos (random):   \", np.mean(random_cos))\n"
      ],
      "metadata": {
        "id": "UpVFgn5DKbD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unpack IRs & Ambiences"
      ],
      "metadata": {
        "id": "geK03XscIk4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augments"
      ],
      "metadata": {
        "id": "YVpqBkMPKuJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "obsolete"
      ],
      "metadata": {
        "id": "YqXbi5x1ug-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample usage, don't run"
      ],
      "metadata": {
        "id": "AtH_PimRm6SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "debug (obsolete)"
      ],
      "metadata": {
        "id": "Duz3MMQQnmbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics & Analysis"
      ],
      "metadata": {
        "id": "Lq8OaxkNK0bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reusable clssifier head MLP/Conv1D"
      ],
      "metadata": {
        "id": "l__1JcyMLA3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenL3 Backbone"
      ],
      "metadata": {
        "id": "63sZosYPLaZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train OpenL3 plus head"
      ],
      "metadata": {
        "id": "UbFJ8spdLhUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AudioMAE Backbone"
      ],
      "metadata": {
        "id": "9o1W5u3bLq3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train AudioMAE plus head"
      ],
      "metadata": {
        "id": "zrxLOvD1LyYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional save heads"
      ],
      "metadata": {
        "id": "FPEduiUrL8aP"
      }
    }
  ]
}